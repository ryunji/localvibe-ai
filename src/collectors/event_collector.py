"""
ì „ì‹œíšŒ/ì´ë²¤íŠ¸ ì •ë³´ ìˆ˜ì§‘ ë° AI ì²˜ë¦¬ ì‹œìŠ¤í…œ
í•„ìš”í•œ íŒ¨í‚¤ì§€: pip install requests beautifulsoup4 pandas anthropic
"""

import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
from datetime import datetime
from typing import List, Dict
import time

class EventCollector:
    def __init__(self):
        self.events = []
        
    # 1. ê³µê³µë°ì´í„° API í™œìš©
    def fetch_public_api_data(self, api_key: str = None):
        """
        ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€ ê³µê³µë°ì´í„° API ì˜ˆì‹œ
        ì‹¤ì œ ì‚¬ìš©ì‹œ https://www.data.go.kr/ ì—ì„œ API í‚¤ ë°œê¸‰ í•„ìš”
        """
        if not api_key:
            print("âš ï¸ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. data.go.krì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.")
            return []
        
        # ê³µì—°ì „ì‹œ ì •ë³´ API ì—”ë“œí¬ì¸íŠ¸ ì˜ˆì‹œ
        url = "http://www.culture.go.kr/openapi/rest/publicperformancedisplays/area"
        
        params = {
            'serviceKey': api_key,
            'keyword': 'ì „ì‹œ',
            'sortStdr': '1',  # ë“±ë¡ì¼ìˆœ
            'numOfRows': '100',
            'pageNo': '1'
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                # XML ë˜ëŠ” JSON íŒŒì‹± (APIì— ë”°ë¼ ë‹¤ë¦„)
                print("âœ… ê³µê³µë°ì´í„° API í˜¸ì¶œ ì„±ê³µ")
                # ì‹¤ì œ íŒŒì‹± ë¡œì§ ì¶”ê°€ í•„ìš”
                return self._parse_public_api(response.text)
            else:
                print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
        except Exception as e:
            print(f"âŒ API ì˜¤ë¥˜: {e}")
        
        return []
    
    def _parse_public_api(self, data):
        """API ì‘ë‹µ íŒŒì‹± (í˜•ì‹ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)"""
        # ì˜ˆì‹œ ë°ì´í„° êµ¬ì¡°
        return []
    
    # 2. ì›¹ ìŠ¤í¬ë˜í•‘
    def scrape_museum_websites(self):
        """ì£¼ìš” ë¯¸ìˆ ê´€/ê°¤ëŸ¬ë¦¬ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
        events = []
        
        # ì˜ˆì‹œ: ì„œìš¸ì‹œë¦½ë¯¸ìˆ ê´€
        try:
            url = "https://sema.seoul.go.kr/kr/exhibition/exhibitionNow"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ selector ìˆ˜ì • í•„ìš”
                exhibition_items = soup.select('.exhibition-item')  # ì˜ˆì‹œ selector
                
                for item in exhibition_items:
                    event = {
                        'title': item.select_one('.title').text.strip() if item.select_one('.title') else '',
                        'location': 'ì„œìš¸ì‹œë¦½ë¯¸ìˆ ê´€',
                        'date': item.select_one('.date').text.strip() if item.select_one('.date') else '',
                        'source': 'sema',
                        'url': url
                    }
                    events.append(event)
                
                print(f"âœ… ì„œìš¸ì‹œë¦½ë¯¸ìˆ ê´€: {len(exhibition_items)}ê°œ ì „ì‹œ ìˆ˜ì§‘")
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        
        # ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë“¤ ì¶”ê°€
        events.extend(self._scrape_mmca())  # êµ­ë¦½í˜„ëŒ€ë¯¸ìˆ ê´€
        events.extend(self._scrape_galleries())  # ê¸°íƒ€ ê°¤ëŸ¬ë¦¬
        
        return events
    
    def _scrape_mmca(self):
        """êµ­ë¦½í˜„ëŒ€ë¯¸ìˆ ê´€ ìŠ¤í¬ë˜í•‘"""
        # êµ¬í˜„ ì˜ˆì‹œ
        return []
    
    def _scrape_galleries(self):
        """ê¸°íƒ€ ê°¤ëŸ¬ë¦¬ ìŠ¤í¬ë˜í•‘"""
        # êµ¬í˜„ ì˜ˆì‹œ
        return []
    
    # 3. í‹°ì¼“ í”Œë«í¼ ìŠ¤í¬ë˜í•‘
    def scrape_ticket_platforms(self):
        """ì¸í„°íŒŒí¬, ì˜ˆìŠ¤24 ë“± í‹°ì¼“ í”Œë«í¼"""
        events = []
        
        try:
            # ì¸í„°íŒŒí¬ ì „ì‹œ í˜ì´ì§€ ì˜ˆì‹œ
            url = "http://ticket.interpark.com/TPGoodsList.asp?Ca=Ar"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
                items = soup.select('.goodsBox')  # ì˜ˆì‹œ selector
                
                for item in items[:50]:  # ìµœëŒ€ 50ê°œ
                    event = {
                        'title': item.select_one('.goodsName').text.strip() if item.select_one('.goodsName') else '',
                        'location': item.select_one('.placeName').text.strip() if item.select_one('.placeName') else '',
                        'date': item.select_one('.playDate').text.strip() if item.select_one('.playDate') else '',
                        'source': 'interpark',
                        'url': url
                    }
                    events.append(event)
                
                print(f"âœ… ì¸í„°íŒŒí¬: {len(items)}ê°œ ì „ì‹œ ìˆ˜ì§‘")
        except Exception as e:
            print(f"âŒ í‹°ì¼“ í”Œë«í¼ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        
        return events
    
    def collect_all_data(self, api_key: str = None):
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
        print("ğŸ” ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...\n")
        
        all_events = []
        
        # 1. ê³µê³µ API
        print("1ï¸âƒ£ ê³µê³µë°ì´í„° API ìˆ˜ì§‘ ì¤‘...")
        all_events.extend(self.fetch_public_api_data(api_key))
        time.sleep(1)
        
        # 2. ì›¹ ìŠ¤í¬ë˜í•‘
        print("\n2ï¸âƒ£ ë¯¸ìˆ ê´€/ê°¤ëŸ¬ë¦¬ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        all_events.extend(self.scrape_museum_websites())
        time.sleep(1)
        
        # 3. í‹°ì¼“ í”Œë«í¼
        print("\n3ï¸âƒ£ í‹°ì¼“ í”Œë«í¼ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        all_events.extend(self.scrape_ticket_platforms())
        
        self.events = all_events
        print(f"\nâœ… ì´ {len(all_events)}ê°œ ì´ë²¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return all_events


class AIEventProcessor:
    """AIë¥¼ í™œìš©í•œ ì´ë²¤íŠ¸ ë°ì´í„° ì²˜ë¦¬"""
    
    def __init__(self, api_key: str = None):
        """
        Anthropic API í‚¤ í•„ìš” (https://console.anthropic.com/)
        ë˜ëŠ” OpenAI ë“± ë‹¤ë¥¸ AI API ì‚¬ìš© ê°€ëŠ¥
        """
        self.api_key = api_key
    
    def deduplicate_events(self, events: List[Dict]) -> List[Dict]:
        """AIë¡œ ì¤‘ë³µ ì´ë²¤íŠ¸ ì œê±°"""
        if not events:
            return []
        
        print("\nğŸ¤– AIë¡œ ì¤‘ë³µ ì œê±° ì¤‘...")
        
        # ê°„ë‹¨í•œ ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ë°˜)
        unique_events = {}
        for event in events:
            title = event.get('title', '').strip()
            if title and title not in unique_events:
                unique_events[title] = event
        
        print(f"âœ… {len(events)} -> {len(unique_events)}ê°œë¡œ ì¤‘ë³µ ì œê±°")
        return list(unique_events.values())
    
    def structure_event_data(self, raw_event: Dict) -> Dict:
        """ë¹„ì •í˜• ë°ì´í„°ë¥¼ êµ¬ì¡°í™”"""
        # AI API í˜¸ì¶œ ëŒ€ì‹  ê¸°ë³¸ êµ¬ì¡°í™”
        structured = {
            'id': hash(raw_event.get('title', '') + raw_event.get('location', '')),
            'title': raw_event.get('title', ''),
            'location': raw_event.get('location', ''),
            'date_range': raw_event.get('date', ''),
            'category': self._categorize_event(raw_event.get('title', '')),
            'source': raw_event.get('source', ''),
            'url': raw_event.get('url', ''),
            'created_at': datetime.now().isoformat()
        }
        return structured
    
    def _categorize_event(self, title: str) -> str:
        """ì´ë²¤íŠ¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        title_lower = title.lower()
        if any(word in title_lower for word in ['ë¯¸ìˆ ', 'ì „ì‹œ', 'ì‘í’ˆ', 'ê°¤ëŸ¬ë¦¬']):
            return 'ë¯¸ìˆ ì „ì‹œ'
        elif any(word in title_lower for word in ['ê³µì—°', 'ì½˜ì„œíŠ¸', 'ë®¤ì§€ì»¬']):
            return 'ê³µì—°'
        elif any(word in title_lower for word in ['ì¶•ì œ', 'í˜ìŠ¤í‹°ë²Œ']):
            return 'ì¶•ì œ'
        else:
            return 'ê¸°íƒ€'
    
    def process_all_events(self, events: List[Dict]) -> List[Dict]:
        """ëª¨ë“  ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        print("\nğŸ”§ AI ì²˜ë¦¬ ì‹œì‘...\n")
        
        # ì¤‘ë³µ ì œê±°
        unique_events = self.deduplicate_events(events)
        
        # êµ¬ì¡°í™”
        print("\nğŸ“Š ë°ì´í„° êµ¬ì¡°í™” ì¤‘...")
        structured_events = [
            self.structure_event_data(event) 
            for event in unique_events
        ]
        
        print(f"âœ… {len(structured_events)}ê°œ ì´ë²¤íŠ¸ êµ¬ì¡°í™” ì™„ë£Œ")
        return structured_events


class EventRecommender:
    """ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ ì‹œìŠ¤í…œ"""
    
    def recommend_by_preference(self, events: List[Dict], user_prefs: Dict) -> List[Dict]:
        """ì‚¬ìš©ì ì„ í˜¸ë„ ê¸°ë°˜ ì¶”ì²œ"""
        preferred_category = user_prefs.get('category', 'ë¯¸ìˆ ì „ì‹œ')
        preferred_location = user_prefs.get('location', '')
        
        scored_events = []
        for event in events:
            score = 0
            
            # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
            if event.get('category') == preferred_category:
                score += 10
            
            # ìœ„ì¹˜ ë§¤ì¹­
            if preferred_location and preferred_location in event.get('location', ''):
                score += 5
            
            scored_events.append({**event, 'score': score})
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_events.sort(key=lambda x: x['score'], reverse=True)
        return scored_events[:10]
    
    def natural_language_search(self, events: List[Dict], query: str) -> List[Dict]:
        """ìì—°ì–´ ê²€ìƒ‰"""
        query_lower = query.lower()
        results = []
        
        for event in events:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
            if (query_lower in event.get('title', '').lower() or
                query_lower in event.get('location', '').lower()):
                results.append(event)
        
        return results


# ì‹¤í–‰ ì˜ˆì‹œ
def main():
    print("=" * 60)
    print("ğŸ¨ ì „ì‹œíšŒ/ì´ë²¤íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # 1-3. ë°ì´í„° ìˆ˜ì§‘
    collector = EventCollector()
    raw_events = collector.collect_all_data(api_key=None)  # API í‚¤ ì…ë ¥
    
    # 4. AI ì²˜ë¦¬
    processor = AIEventProcessor()
    structured_events = processor.process_all_events(raw_events)
    
    # ë°ì´í„° ì €ì¥
    df = pd.DataFrame(structured_events)
    df.to_csv('events_data.csv', index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: events_data.csv")
    
    # ì¶”ì²œ ì‹œìŠ¤í…œ ì˜ˆì‹œ
    recommender = EventRecommender()
    user_prefs = {
        'category': 'ë¯¸ìˆ ì „ì‹œ',
        'location': 'ì„œìš¸'
    }
    
    recommendations = recommender.recommend_by_preference(structured_events, user_prefs)
    print(f"\nğŸ¯ ì¶”ì²œ ì´ë²¤íŠ¸ (ìƒìœ„ {len(recommendations)}ê°œ):")
    for i, event in enumerate(recommendations[:5], 1):
        print(f"{i}. {event['title']} - {event['location']}")
    
    # ìì—°ì–´ ê²€ìƒ‰ ì˜ˆì‹œ
    search_results = recommender.natural_language_search(structured_events, "í™ëŒ€")
    print(f"\nğŸ” 'í™ëŒ€' ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")

if __name__ == "__main__":
    main()